SimpleViT_Factorized(
  (to_patch_embedding): Sequential(
    (0): Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1=4, p2=4)
    (1): Linear(in_features=48, out_features=128, bias=True)
  )
  (transformer): Transformer(
    (layers): ModuleList(
      (0-3): 4 x ModuleList(
        (0): Attention(
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attend): Softmax(dim=-1)
          (to_q): Sequential(
            (0): Linear(in_features=128, out_features=8, bias=False)
            (1): Linear(in_features=8, out_features=256, bias=False)
          )
          (to_k): Sequential(
            (0): Linear(in_features=128, out_features=8, bias=False)
            (1): Linear(in_features=8, out_features=256, bias=False)
          )
          (to_v): Sequential(
            (0): Linear(in_features=128, out_features=8, bias=False)
            (1): Linear(in_features=8, out_features=256, bias=False)
          )
          (to_out): Sequential(
            (0): Linear(in_features=256, out_features=8, bias=False)
            (1): Linear(in_features=8, out_features=128, bias=False)
          )
        )
        (1): FeedForward(
          (net): Sequential(
            (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=128, out_features=8, bias=True)
            (2): Linear(in_features=8, out_features=128, bias=True)
            (3): GELU(approximate='none')
            (4): Linear(in_features=128, out_features=8, bias=True)
            (5): Linear(in_features=8, out_features=128, bias=True)
          )
        )
      )
    )
  )
  (to_latent): Identity()
  (linear_head): Sequential(
    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=128, out_features=10, bias=True)
  )
)
